{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "In this notebook, I implement a Hidden Markov Model (HMM) with Gaussian Response variables based on text and code examples of Zucchini and MacDonald [1].\n",
    "\n",
    "HMMs are characterized by the following [2]:\n",
    "\n",
    "1. They try to infer **hidden** or **latent** states from discrete or continuous time-series data. Consider the image below. On the x-axis we have time, so this is a time-series dataset. On the y-axis we have four sleep stages (light, deep, REM sleep and wakefulness). These are the latent stages. My fitbit attempts to extract these stages based on my movement and heart rate for each point in the time-series. This results in a sequence of latent states. For the image below, that would look something like this: (Awake, REM, REM, REM, ..., Deep, Deep, REM, Deep, Deep, Awake, ...) and so on.\n",
    "\n",
    "<img src=\"img/sleeeeeeping.PNG\" width=\"500\">\n",
    "\n",
    "2. They are **memoryless** models [3]. When predicting the state at time point $t$, these models only look at the state at $t-1$ to infer the probability of transitioning into a next state. If the state at $t-1$ is REM, then what is the probability of observing the next state as REM, light, deep sleep or wakefulness? This is also called the *Markov assumption*.\n",
    "\n",
    "3. Each state has its own **component distribution**, and the joint distribution of these component distribution is a **mixture distribution**. All this means is that we think that REM sleep, light sleep, deep sleep and wakefulness have different distributions when we consider movement and heart rate. This is easy enough to believe if you think about these variables; a person who is awake typically has a heart rate between 60-100 BPM, whereas a person in deep sleep has a heart rate of 50-60 BPM. If we believed these variables to be normally distributed, then we could model the heart rate component distributions using the means 75 for wakefulness and 55 for deep sleep. Depending on which heart rate we observe at each time step, we can compute the probability of seeing that observation for each of the component distributions. This procedure is how we translate observed data to latent states: which state typically fits this observed value best? \n",
    "\n",
    "Note that the mixture distribution in (3) is not independent. In independent mixture distributions (e.g. multivariate normal distribution) the observations are drawn without conditioning on a third variable (for example a sleep state at a specific point in time). In our case, however, the observations are drawn based on the previous state, and these probabilities are different for each state (this is the markov assumption mentioned in (2)). For example, it is not likely that you will go from wakefulness into REM sleep immediately, so the probability of seeing this state after observing a wakeful state is low. Conversely, it is very likely that you will go from light sleep to REM, so the probability associated with this transition is much higher.\n",
    "\n",
    "### Notation\n",
    "\n",
    "I use the following notation in this notebook, based on [1] and [2]. \n",
    "\n",
    "#### Observed data\n",
    "\n",
    "For a time-series sequence of length $T$, let the observed outcome data (e.g. movement/heart rate) be denoted by:\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "X_{1:T} := (X_1, X_2, \\dots, X_T)\n",
    "$$\n",
    "\n",
    "#### Component distributions\n",
    "\n",
    "Let $m$ denote the number of components in the HMM. Then the component of the HMM at time $t$ is given by:\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "C_t = i, i \\in \\{1, 2, \\dots, m\\}\n",
    "$$ \n",
    "\n",
    "The distribution $X_t$ of the observed data is considered to be dependent on the state $C_t$ (see point 3 above), so that we can write: \n",
    "\n",
    "$$\n",
    "\\tag{3}\n",
    "p_i(X_t) := p(X_t | C_t = i)\n",
    "$$ \n",
    "\n",
    "#### Marginal distributions\n",
    "\n",
    "The marginal distribution (collapsed over all the states/components) is given by the following mixture distribution:\n",
    "\n",
    "$$\n",
    "\\tag{4}\n",
    "p(X_t) = \\sum_{i=1}^m \\delta_i p_i(X_t)\n",
    "$$\n",
    "\n",
    "where $\\delta_i$ represents the proportion of observations in component $i$. If 30/100 observations are in the REM sleep component and 70/100 are in the wakefulness component, then the components of these distributions are $\\vec{\\delta} = [0.3, 0.7]$. This vector sums to unity $\\sum \\delta_i = 1$\n",
    "\n",
    "#### Transition probabilities\n",
    "\n",
    "The probability of observing $C_t$ at timestep $t$ is dependent on the state observed in the previous timestep. (see point 2 above):\n",
    "\n",
    "$$\n",
    "\\tag{5}\n",
    "p(C_t | C_1, \\dots, C_{t-1}) = p(C_t|C_{t-1})\n",
    "$$\n",
    "\n",
    "These probabilities are called **transition probabilities** and govern the probability of moving from state $C_t$ to $C_{t+1}$ at timestep $t$ (e.g. from REM to deep sleep). They are collected in the **transition probability matrix** (TPM) $\\pmb{\\Gamma}(t) \\in \\mathbb{R}^{m \\times m}$ such that:\n",
    "\n",
    "$$\n",
    "\\tag{6}\n",
    "\\gamma_{ij}(t) := p(C_{t+1} =j | C_t = i), \\ \\ \\forall i,j = 1, \\dots, m\n",
    "$$\n",
    "\n",
    "The row-sums of the TPM sum to unity:\n",
    "\n",
    "$$\n",
    "\\tag{7}\n",
    "\\sum_{j=1}^m \\gamma_{ij}(t) = 1\n",
    "$$\n",
    "\n",
    "If the transition probabilities are time-homogenous, meaning that they are the same for each timestep $t \\in T$, then $\\pmb{\\Gamma}(t) = \\pmb{\\Gamma}$.\n",
    "\n",
    "#### Stationary distribution\n",
    "\n",
    "If the HMM is **stationary** this means that the probability distribution of the $m$ components converges as $t \\to \\infty$. This vector is denoted by $\\vec{\\zeta} \\in \\mathbb{R}^{m \\times 1}$ and it can be obtained by solving\n",
    "\n",
    "$$\n",
    "\\tag{8}\n",
    "\\vec{\\zeta} \\cdot \\pmb{\\Gamma} = \\vec{\\zeta}\n",
    "$$\n",
    "\n",
    "This vector sums to unity: $\\sum \\zeta_i = 1$\n",
    "\n",
    "#### Initial distribution\n",
    "\n",
    "At time $t=1$, we must supply an initial probability distribution. This distribution is denoted by $\\vec{\\pi} \\in \\mathbb{R}^{m \\times 1}$:\n",
    "\n",
    "$$\n",
    "\\tag{9}\n",
    "\\pi_i := p(C_1 =i), \\ \\ \\forall i \\in 1, \\dots, m \n",
    "$$\n",
    "\n",
    "The elements of $\\vec{\\pi}$ determine in which state the HMM starts. We can then compute the probability distribution over the components at time $t$ as:\n",
    "\n",
    "$$\n",
    "\\tag{10}\n",
    "p(C_t = 1, C_t = 2, \\dots, C_t = m) = \\vec{\\pi} \\cdot \\pmb{\\Gamma}^{t-1} = \\vec{\\pi}(t)\n",
    "$$\n",
    "\n",
    "When the HMM is stationary, the initial state vector $\\vec{\\pi} = \\vec{\\zeta}$ \n",
    "\n",
    "## Simulating data for Gaussian response distributions\n",
    "\n",
    "To simulate data suitable for an HMM model, we first create a sequence of observations based on a 'true' TPM and initial distribution. Then, depending on the state at each timestep $t$, we draw an observation from its component distribution.\n",
    "\n",
    "### Simulating a hidden sequence\n",
    "\n",
    "The code below simulates a sequence for $T=100$ for two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of components \n",
    "m = 2\n",
    "# Timesteps t\n",
    "T = 100\n",
    "# Set seed\n",
    "np.random.seed(563)\n",
    "# Transition matrix\n",
    "gamma = np.array([[0.33, 0.67],\n",
    "                  [0.89, 0.11]])\n",
    "# Initial states\n",
    "pi = np.array([0.71, 0.29])\n",
    "# Draw from one of the states with probability pi_i\n",
    "# Generate sequence\n",
    "seq = np.zeros(T, dtype = np.int32)\n",
    "for t in range(T):\n",
    "    if t == 0:\n",
    "        # Draw from the initial distribution\n",
    "        seq[t] = int(np.nonzero(np.random.multinomial(1, pi))[0])\n",
    "    else:\n",
    "        # Multiply the initial distribution with the TPM\n",
    "        prob_states = np.dot(pi, np.linalg.matrix_power(gamma, t))\n",
    "        seq[t] = int(np.nonzero(np.random.multinomial(1, prob_states))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating observed data \n",
    "\n",
    "In the code below, we define two normally distributed component distributions:\n",
    "\n",
    "1. $C = 1 \\sim N(7, 2)$\n",
    "2. $C = 2 \\sim N(15, 4)$\n",
    "\n",
    "For each timestep $t$, we check which component we have and then draw an observation from its component distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate\n",
    "X = np.zeros(T, dtype = np.float32)\n",
    "# Set seed\n",
    "np.random.seed(886)\n",
    "# mu / sd of the component distribution\n",
    "mu = np.array([7,15])\n",
    "sd = np.array([2,4])\n",
    "# Populate\n",
    "for i in range(T):\n",
    "    # Get state\n",
    "    state = seq[i]\n",
    "    # Draw from normal distribution\n",
    "    X[i] = np.round(np.random.normal(mu[state], sd[state]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasper/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6571: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9f60a92320>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot observed data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.distplot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the state sequence\n",
    "\n",
    "We now turn to estimating the state sequence from the observed data $X$ that we drew in the above code block. \n",
    "\n",
    "### Marginal distribution of $X$ and log-likelihood\n",
    "\n",
    "In any directed graphical model, the joint distribution of a set of random variables $V_i$ is given by [1, pp. 33-35]:\n",
    "\n",
    "$$\n",
    "\\tag{11}\n",
    "p(V_1, V_2, \\dots, V_n) = \\prod_{i=1}^n p(V_i | pa(V_i))\n",
    "$$\n",
    "\n",
    "where $pa(V_i)$ denotes the 'parents' of $V_i$. See [this document](http://www.cs.columbia.edu/~blei/fogm/2015F/notes/graphical-models.pdf) for further details. \n",
    "\n",
    "For the random variables $X_t, X_{t+k}, C_t, C_{t+k}$, we can take the marginal by taking over the state sequences to get:\n",
    "\n",
    "$$\n",
    "\\tag{12}\n",
    "p(X_t = u, X_{t+k} = w) = \\sum_{i=1}^m \\sum_{j=1}^m p(C_t = i) p_i(v) p(C_{t+k} = j | C_t = i) p_j(w)\n",
    "$$\n",
    "\n",
    "In $(12)$, we can discern the following elements:\n",
    "\n",
    "1. Recall that $\\vec{\\pi}(t) = \\vec{\\pi} \\cdot \\pmb{\\Gamma}^{t-1}$, so that $p(C_t = i) = \\vec{\\pi}(t)$.\n",
    "2. We let \n",
    "\n",
    "$$\n",
    "p_i(v) = \\text{diag}(p_i(v)) = \\pmb{P}(v)\\text{, and}\\\\ \n",
    "p_j(w) = \\text{diag}(p_j(w)) = \\pmb{P}(w)\n",
    "$$ \n",
    "\n",
    "3. We see that $p(C_{t+k} = j | C_t = i) p_j(w) = \\gamma_{ij}(k)$ and that $\\sum_{i=1}^m \\sum_{j=1}^m \\gamma_{ij}(k) = \\pmb{\\Gamma}^k$\n",
    "\n",
    "Then it follows that:\n",
    "\n",
    "$$\n",
    "\\tag{13}\n",
    "p(X_t = u, X_{t+k} = w) = \\vec{\\pi} \\pmb{\\Gamma}^{t-1} \\pmb{P}(v) \\pmb{\\Gamma}^k \\pmb{P}(w) \\vec{1}\n",
    "$$\n",
    "\n",
    "Where $\\vec{1}$ is an $m \\times 1$ column vector of ones.\n",
    "\n",
    "The result is a scalar which, at the final timestep, contains the likelihood of the HMM sequence.\n",
    "\n",
    "### Forward algorithm\n",
    "\n",
    "The algorithm we use to compute the likelihood is called the **forward algorithm**. Note that we are using the actual parameters (i.e. these should be unobserved) to compute the hidden state sequence).\n",
    "\n",
    "The forward algorithm works as follows:\n",
    "\n",
    "> 1. Define an initial distribution $\\vec{\\pi}$, transition probability matrix $\\pmb{\\Gamma}$ number of components $m$ and component distribution parameters $\\vec{\\mu} = [\\mu_1, \\dots, \\mu_m]$ and $\\vec{\\sigma} = [\\sigma_1, \\dots, \\sigma_m]$. Initialize a vector $S \\in \\mathbb{N}^{T \\times 1}$ to hold the predicted state sequences.\n",
    "> 2. Set $\\vec{\\alpha} = \\vec{\\pi}$ and set $S_1 := \\text{arg max} \\ \\vec{\\alpha}$.\n",
    "> 3. For $x_t$ in $X$, do:\n",
    ">     1. Compute the p.d.f. of $x_t$ for each of the component distributions $p_i$. \n",
    ">     2. Store the values of the component distributions in a matrix $\\pmb{P}$.\n",
    ">     3. Set the vector $\\vec{\\alpha}$ to be $\\vec{\\alpha}_t := \\vec{\\alpha}_{t-1} \\pmb{\\Gamma} \\pmb{P}$\n",
    ">     4. Set $S_t = \\text{arg max} \\ \\vec{\\alpha}_t$\n",
    "> 4. At the final timestep $T$, compute the log-likelihood $L = \\vec{\\alpha}_t \\vec{1}$\n",
    "\n",
    "This algorithm is implemented in python code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward algorithm\n",
    "from scipy.stats import norm\n",
    "\n",
    "# We use the initial probabilities pi\n",
    "# NOTE: we shouldn't really know these ...\n",
    "\n",
    "# Initialize alpha using the initial distribution\n",
    "alpha = pi\n",
    "# Initialize states to 0\n",
    "states = np.zeros(T, dtype = np.int32)\n",
    "# For each observation and timestep t, compute the state using the forward algorithm\n",
    "for t, x in enumerate(X):\n",
    "    # Compute probabilities for x at timestep t\n",
    "    # Note that we are not assuming stationarity because we have alpha_1 = pi * P \n",
    "    #  at t == 1\n",
    "    # Note that we are using population values mu and sd here\n",
    "    p = norm.pdf(x, mu, sd)\n",
    "    # Convert to diagonal matrix\n",
    "    P = np.diag(p)\n",
    "    # Form the matrix B\n",
    "    B = np.dot(gamma, P)\n",
    "    # Update alpha\n",
    "    alpha = np.dot(alpha, B)\n",
    "    # Add state\n",
    "    states[t] = np.argmax(alpha)\n",
    "    # At the last step, dot with 1' to get the likelihood\n",
    "    if t == (T-1):\n",
    "        L = np.dot(alpha, np.ones((m, 1), dtype = np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 observations ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted  actual\n",
       "0          1       1\n",
       "1          0       1\n",
       "2          1       1\n",
       "3          0       0\n",
       "4          1       0\n",
       "5          0       0\n",
       "6          0       0\n",
       "7          0       0\n",
       "8          1       1\n",
       "9          0       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack predicted and actual states column-wise\n",
    "import pandas as pd\n",
    "seqs = np.vstack((states,seq)).T\n",
    "# Cast to data frame\n",
    "print(\"First 10 observations ...\")\n",
    "pd.DataFrame(seqs, columns = [\"predicted\", \"actual\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: [8.14757306e-129]\n",
      "Alphas at time T: [4.30438168e-129 3.84319137e-129]\n"
     ]
    }
   ],
   "source": [
    "# Print likelihood and alphas\n",
    "print(\"Likelihood: {}\".format(L))\n",
    "print(\"Alphas at time T: {}\".format(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above implementation of the forward algorithm, clearly we are running into the issue of underflow. The values for the alphas are getting very small, and we need to start using some computational tricks to make sure we can handle longer sequences.\n",
    "\n",
    "### Scaling the forward probabilities\n",
    "\n",
    "To avoid underflow, we scale the forward probabilities $\\vec{\\alpha}(t)$ by the sum of this vector at time $t$:\n",
    "\n",
    "$$\n",
    "\\tag{14}\n",
    "\\vec{\\phi}(t) := \\frac{\\vec{\\alpha}(t)}{\\sum_{i=1}^m \\alpha_{i}(t)}\n",
    "$$\n",
    "\n",
    "This has the effect of scaling the forward probabilities to values s.t. $\\sum_{i=1}^m \\phi_i(t) = 1$. \n",
    "\n",
    "### Computing the log-likelihood instead of the likelihood\n",
    "\n",
    "If we let $\\sum_{i=1}^m \\alpha_{i}(t) = w_t$, then instead of computing $L = \\vec{\\alpha}_t \\vec{1}$ at time $T$, we can now set $L = w_T$. However, this value can still get very small, so the natural thing to do is to compute the log-likelihood instead.\n",
    "\n",
    "The likelihood in $(14)$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\tag{15}\n",
    "L = w_T = \\prod_{t=1}^T \\frac{w_t}{w_{t-1}}\n",
    "$$\n",
    "\n",
    "Taking the log of $(15)$ yields:\n",
    "\n",
    "$$\n",
    "\\tag{16}\n",
    "\\log{L} = LL = \\sum_{t=1}^T \\log{\\frac{w_t}{w_{t-1}}} = \\sum_{t=1}^T \\log{\\vec{\\phi}_{t-1} \\pmb{\\Gamma} \\pmb{P} \\vec{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward algorithm\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Initialize alphas using the initial distribution\n",
    "# scale the distribution by the sum of the alphas to prevent underflow\n",
    "ones = np.ones((m, 1), dtype = np.int32)\n",
    "phi = pi / np.dot(pi, ones) # ==> this is just scaling by 1 \n",
    "# Initialize states to 0\n",
    "states = np.zeros(T, dtype = np.int32)\n",
    "# Initialize log-likelihood to 0\n",
    "L = 0\n",
    "# For each observation, compute the state using the forward algorithm\n",
    "for t, x in enumerate(X):\n",
    "    # Compute probabilities for x at timestep t\n",
    "    p = norm.pdf(x, mu, sd)\n",
    "    # Convert to diagonal matrix\n",
    "    P = np.diag(p)\n",
    "    # Form the matrix B\n",
    "    B = np.dot(gamma, P)\n",
    "    # Create the vector v as the product of scaled alphas and B\n",
    "    v = np.dot(phi, B)\n",
    "    # Create the scalar u as the sum of the scaled alphas\n",
    "    u = np.dot(v, ones)\n",
    "    # Add to log-likelihood\n",
    "    L += np.log(u)\n",
    "    # Scale vector by the sum of v to create new phi\n",
    "    phi = v / u\n",
    "    # Add state\n",
    "    states[t] = np.argmax(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 observations ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted  actual\n",
       "0          1       1\n",
       "1          0       1\n",
       "2          1       1\n",
       "3          0       0\n",
       "4          1       0\n",
       "5          0       0\n",
       "6          0       0\n",
       "7          0       0\n",
       "8          1       1\n",
       "9          0       0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack predicted and actual states column-wise\n",
    "import pandas as pd\n",
    "seqs = np.vstack((states,seq)).T\n",
    "# Cast to data frame\n",
    "print(\"First 10 observations ...\")\n",
    "pd.DataFrame(seqs, columns = [\"predicted\", \"actual\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood: [-294.9357569]\n",
      "Phi at time T: [0.52830231 0.47169769]\n"
     ]
    }
   ],
   "source": [
    "# Print likelihood and alphas\n",
    "print(\"Log-Likelihood: {}\".format(L))\n",
    "print(\"Phi at time T: {}\".format(phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above algorithm is defined as the function `HMM_forward` in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM_forward(X: np.array, pi: np.array, gamma: np.array, \n",
    "                mu: np.array, sd: np.array, return_states = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the forward step of an HMM\n",
    "    \n",
    "    :param X: n x 1 numpy array containing the observed data\n",
    "    :param pi: m x 1 numpy array containing initial values\n",
    "    :param gamma: m x m numpy array containing transition probabilities\n",
    "    :param mu: m x 1 numpy array containing means of component distributions\n",
    "    :param sd: m x 1 numpy array containing sds of component distributions\n",
    "    :param return_states: boolean. If True, then the state sequence is returned\n",
    "    \n",
    "    :return: Log-Likelihood and, if desired, the state sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    # For dimensions\n",
    "    m = mu.shape[0]\n",
    "    T = X.shape[0]\n",
    "    \n",
    "    # Initialize alphas using the initial distribution\n",
    "    # scale the distribution by the sum of the alphas to prevent underflow\n",
    "    ones = np.ones((m, 1), dtype = np.int32)\n",
    "    phi = pi \n",
    "    # Initialize states to 0\n",
    "    if return_states: states = np.zeros(T, dtype = np.int32)\n",
    "    # Initialize log-likelihood to 0\n",
    "    L = 0\n",
    "    # For each observation, compute the state using the forward algorithm\n",
    "    for t, x in enumerate(X):\n",
    "        # Compute probabilities for x at timestep t\n",
    "        p = norm.pdf(x, mu, sd)\n",
    "        # Convert to diagonal matrix\n",
    "        P = np.diag(p)\n",
    "        # Form the matrix B\n",
    "        B = np.dot(gamma, P)\n",
    "        # Create the vector v as the product of scaled alphas and B\n",
    "        v = np.dot(phi, B)\n",
    "        # Create the scalar u as the sum of the scaled alphas\n",
    "        u = np.dot(v, ones)\n",
    "        # Add to log-likelihood\n",
    "        L += np.log(u)\n",
    "        # Scale vector by the sum of v to create new phi\n",
    "        phi = v / u\n",
    "        # Add state\n",
    "        if return_states: states[t] = np.argmax(phi)\n",
    "        \n",
    "    # Return\n",
    "    if return_states:\n",
    "        return((L, states))\n",
    "    else:\n",
    "        return(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using an unconstrained optimizer\n",
    "\n",
    "Given the observed data $X$, we want to find those values of the parameters (means, variances, transition probabilities) that maximize the likelihood of the state sequence. This is an optimization problem that we can solve using scipy's `minimize` function.\n",
    "\n",
    "### Dealing with constraints\n",
    "\n",
    "For the HMM with normal distribution we have no real constraints on the parameters $\\mu_1, \\mu_2, \\sigma^2_1, \\sigma^2_2$ since they are continuous and can take on any value. However, we have a constraint on the transition probability matrix $\\pmb{\\Gamma}$ because its rows must sum to one.\n",
    "\n",
    "Given that the TPM has $m \\times (m-1)$ free parameters (since the rows must sum to one), we can redefine the TPM using only the off-diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tpm(gamma: np.array) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform the transition probability matrix gamma using the function g(x) = exp(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply transformation of gamma\n",
    "    # NOTE: have to reshape this as a column vector to make it compatible with R implementation\n",
    "    tgamma = np.log(gamma / np.diag(gamma).reshape((gamma.shape[0],1)))\n",
    "    # Retrieve off-diagonal elements\n",
    "    # NOTE: have to reverse the nonzero elements to make it compatible with R implementation\n",
    "    idx_tau = np.nonzero(tgamma)[::-1]\n",
    "    # Retrieve nonzero elements\n",
    "    tgamma_flat = tgamma[idx_tau]\n",
    "    # Return values of transformation and indices\n",
    "    return({\"tgamma\": tgamma_flat, \"tgamma_idx\": idx_tau})\n",
    "\n",
    "def transform_tgamma(tgamma: dict) -> np.array:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform the transformed values of gamma back to probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get values from dict\n",
    "    tgamma, tgamma_idx = tgamma.values()\n",
    "    # Identity matrix\n",
    "    z = np.eye(tgamma_idx[0].shape[0])\n",
    "    # Fill off-diagonal elements\n",
    "    z[tgamma_idx] = np.exp(tgamma)\n",
    "    # Scale\n",
    "    z /= np.sum(z, axis=1).reshape((z.shape[0], 1))\n",
    "    # Solve for delta (initial values)\n",
    "    delta = np.linalg.solve((np.eye(z.shape[0]) - z + 1).T, np.ones(z.shape[0]))\n",
    "    # Return\n",
    "    return((z, delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this for the true transition matrix, we get the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33 0.67]\n",
      " [0.89 0.11]]\n",
      "[0.57051282 0.42948718]\n"
     ]
    }
   ],
   "source": [
    "# Transform the transition probability matrix\n",
    "a = transform_tpm(gamma)\n",
    "# Transform back and solve for delta\n",
    "gam, delta = transform_tgamma(a)\n",
    "# Print\n",
    "print(gam)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting values\n",
    "\n",
    "To assign starting values for the parameters, we can do the following:\n",
    "\n",
    "1. For means, we can divide the observations into $k$ subsets using quartiles/quantiles and take the mean of the subsets.\n",
    "2. For variances, we can take the variance of the $k$ subsets.\n",
    "3. For initial transition probabilities, we can initialize the matrix $\\pmb{\\Gamma}$ using a uniform distribution with small values (e.g. 0.01 - 0.05) or initialize all values using the same, small value (e.g. 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "gamma_init = np.array([[0.99, 0.01],\n",
    "                       [0.01, 0.99]])\n",
    "# Split observed data at median\n",
    "gr1 = X[X < np.median(X)]\n",
    "gr2 = X[X >= np.median(X)]\n",
    "# Construct means / variances\n",
    "mu = np.array([np.mean(gr1), np.mean(gr2)])\n",
    "sd = np.array([np.sqrt(np.var(gr1)), np.sqrt(np.mean(gr2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.59511985, -4.59511985,  6.25819826, 14.03505325,  1.37487519,\n",
       "        3.74633861])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimize function only takes a single array, so we need to add all parameters to\n",
    "#  a single array\n",
    "tgamma, tgamma_idx = transform_tpm(gamma_init).values()\n",
    "# Concatenate values to flat array\n",
    "x0 = np.concatenate([tgamma, mu, sd])\n",
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below computes the Log-Likelihood given the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def HMM(parameters: np.array, *args: tuple) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Given an array of parameters, compute the log-likelihood of the HMM\n",
    "    \n",
    "    :param parameters: parameters passed to the model. This is a single (flat)\n",
    "                        numpy array whose valuese correspond to the following \n",
    "                        parameters:\n",
    "                         (1) indices 0:(m*(m-1)) ==> transformed values of the \n",
    "                              transition matrix\n",
    "                         (2) indices (m*(m-1)):[(m*(m-1))+m] ==> means of the \n",
    "                              component distributions\n",
    "                         (3) indices [(m*(m-1))+m]:[(m*(m-1))+2*m] ==> variances\n",
    "                              of the component distributions\n",
    "    :param *args: additional parameters passed to the function:\n",
    "                         (1) args[0] ==> number of components\n",
    "                         (2) args[1] ==> tuple of numpy arrays used to transform\n",
    "                                          the transformed gamma values back to\n",
    "                                          their original form\n",
    "                         (3) args[2] ==> T-length numpy array containing observed\n",
    "                                          data\n",
    "                                          \n",
    "    :return: Log-Likelihood of the model given the parameters\n",
    "                                          \n",
    "    :seealso: functions transform_tpm() and transform_tgamma() above\n",
    "    \n",
    "    :seealso: Zucchini, W., MacDonald, I. L., & Langrock, R. (2017). Hidden Markov \n",
    "                             models for time series: an introduction using R. \n",
    "                             Chapman and Hall/CRC. Chapters 2-3\n",
    "    \"\"\"\n",
    "    \n",
    "    # Args is the size of the 2x2 gamma matrix and the tgamma idices to backtransform to gamma\n",
    "    m = args[0]\n",
    "    tgamma_idx = args[1]\n",
    "    X = args[2]\n",
    "    \n",
    "    # Unroll parameters\n",
    "    tgamma = parameters[:(m*(m-1))]\n",
    "    mu = parameters[(m*(m-1)):(m*(m-1)+m)]\n",
    "    sd = parameters[(m*(m-1)+m):(m*(m-1)+2*m)]\n",
    "    \n",
    "    # Transform working parameters to natural parameters \n",
    "    # And solve for delta (initial values)\n",
    "    gamma, pi = transform_tgamma({\"tgamma\": tgamma, \"tgamma_idx\": tgamma_idx})\n",
    "    \n",
    "    # Compute LL\n",
    "    L = HMM_forward(X, pi, gamma, mu, sd, return_states = False)\n",
    "    \n",
    "    # Return log-likelihood\n",
    "    return( -1 * L[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a non-linear Newton-like algorithm to search for a good set of parameters that maximize the hidden state sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimize function from scipy\n",
    "res = minimize(HMM, x0, args = (m, tgamma_idx, X), method = \"L-BFGS-B\",\n",
    "               options = {'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform gamma and pi\n",
    "gamma_pred, pi_pred = transform_tgamma({\"tgamma\":res.x[:2], \"tgamma_idx\":tgamma_idx})\n",
    "# Construct mu, sd\n",
    "mu_pred = res.x[2:4]\n",
    "sd_pred = res.x[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute state sequences\n",
    "_, states_pred = HMM_forward(X, pi_pred, gamma_pred, mu_pred, sd_pred, \n",
    "                             return_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 observations ...\n",
      "Steps correct: 95.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted  actual\n",
       "0          0       0\n",
       "1          0       0\n",
       "2          0       0\n",
       "3          0       0\n",
       "4          1       0\n",
       "5          1       1\n",
       "6          0       0\n",
       "7          1       1\n",
       "8          0       0\n",
       "9          1       1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack predicted and actual states column-wise\n",
    "import pandas as pd\n",
    "seqs = np.vstack((states_pred,seq)).T\n",
    "# Cast to data frame\n",
    "print(\"First 10 observations ...\")\n",
    "print(\"Steps correct: {}%\".format(np.round(sum(states_pred == seq) / len(seq),4) * 100))\n",
    "pd.DataFrame(seqs, columns = [\"predicted\", \"actual\"]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using Expectation-Maximization (EM) \n",
    "\n",
    "The function below is a general, recursive function that can be used to compute the likelihood and state sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM_EM(X: np.array, phi: np.array, gamma: np.array, k: int, **kwargs) -> tuple:\n",
    "    \n",
    "    \"\"\"\n",
    "    Use forward algorithm to compute the state sequence and Log-Likelihood\n",
    "    \n",
    "    :param X: Observed data\n",
    "    :param phi: Initial distribution\n",
    "    :param gamma: Transition Probability Matrix \n",
    "    :param k: Number of hidden states\n",
    "    \n",
    "    :return: tuple containing \n",
    "        (1) state sequence as T-dimensional array\n",
    "        (2) Log-Likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize alphas using the initial distribution\n",
    "    # scale the distribution by the sum of the alphas to prevent underflow\n",
    "    ones = np.ones((k, 1), dtype = np.int32)\n",
    "    # If states not supplied, initialize them\n",
    "    if kwargs.get(\"states\") is None:\n",
    "        # Initialize states to 0\n",
    "        states = np.zeros(X.shape[0], dtype = np.int32)\n",
    "    else:\n",
    "        states = kwargs[\"states\"]\n",
    "    # Initialize log-likelihood to 0 if not passed\n",
    "    if kwargs.get(\"L\") is None:\n",
    "        L = kwargs[\"L\"]\n",
    "    else:\n",
    "        L = 0\n",
    "        \n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Zucchini, W., MacDonald, I. L., & Langrock, R. (2017). Hidden Markov models for time series: an introduction using R. Chapman and Hall/CRC. <br>\n",
    "[2] Visser, I. (2011). Seven things to remember about hidden Markov models: A tutorial on Markovian models for time series. Journal of Mathematical Psychology, 55(6), 403-415. <br>\n",
    "[3] Aarts, E. (2016). Beyond the average: Choosing and improving statistical methods to optimize inference from complex neuroscience data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "HMM",
   "language": "python",
   "name": "hmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
